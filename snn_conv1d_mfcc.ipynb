{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnzcdoR8cdbUW8TdTGjZ8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maya7991/gsc_classification/blob/main/snn_conv1d_mfcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SNN conv1D on MFCC"
      ],
      "metadata": {
        "id": "yUJCgZgiVp5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "ZbeYIsj8VnEt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "yXeIc6I4UtCN"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet\n",
        "!pip install torchaudio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6t4lE-vU32K"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "from snntorch import spikegen, surrogate, functional as SF\n",
        "import snntorch as snn"
      ],
      "metadata": {
        "id": "MA7BCq3vEmxF"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Preprocess the Speech Command Dataset"
      ],
      "metadata": {
        "id": "7dTnG_4pU7Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchaudio.transforms.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=20,\n",
        "    melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 40}\n",
        ")\n",
        "\n",
        "train_dataset = SPEECHCOMMANDS(\n",
        "    \"./\", download=True, subset=\"training\")\n",
        "test_dataset = SPEECHCOMMANDS(\n",
        "    \"./\", download=True, subset=\"testing\")\n",
        "\n",
        "# Limit to a few keywords for now (e.g., \"yes\", \"no\", \"up\", \"down\")\n",
        "keywords = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "label_dict = {k: i for i, k in enumerate(keywords)}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    X, y = [], []\n",
        "    max_len = 0\n",
        "    mfccs = []\n",
        "\n",
        "    for waveform, sample_rate, label, *_ in batch:\n",
        "        if label in keywords:\n",
        "            mfcc = transform(waveform).squeeze(0)  # [n_mfcc, time]\n",
        "            mfccs.append(mfcc)\n",
        "            # print(\"MFCC shape:\", mfcc.shape)  # add this\n",
        "            y.append(label_dict[label])\n",
        "            max_len = max(max_len, mfcc.shape[1])\n",
        "\n",
        "    for mfcc in mfccs:\n",
        "        pad_len = max_len - mfcc.shape[1]\n",
        "        padded = F.pad(mfcc, (0, pad_len))  # Pad on the time dimension (right side)\n",
        "        X.append(padded)\n",
        "\n",
        "    return torch.stack(X), torch.tensor(y)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "oHMX5TFXU-dk"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latency encoding\n",
        "\n",
        "encode mfcc features to spike trains"
      ],
      "metadata": {
        "id": "hq3EDG8tVCm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def encode_input(mfcc_batch, num_steps=100):\n",
        "#     # Normalize to [0, 1]\n",
        "#     data = (mfcc_batch - mfcc_batch.min()) / (mfcc_batch.max() - mfcc_batch.min())\n",
        "#     # [B x C x L] → [B x L] if needed\n",
        "#     data = data.mean(dim=1) if data.ndim == 3 else data\n",
        "#     # Apply latency encoding\n",
        "#     spk_data = spikegen.latency(data, num_steps=num_steps, normalize=True, linear=True)\n",
        "#     return spk_data  # shape: [T x B x L]\n",
        "\n",
        "def encode_input(mfcc_batch, num_steps=100):\n",
        "    # Normalize to [0, 1] per sample\n",
        "    min_val = mfcc_batch.amin(dim=(1,2), keepdim=True)\n",
        "    max_val = mfcc_batch.amax(dim=(1,2), keepdim=True)\n",
        "    data = (mfcc_batch - min_val) / (max_val - min_val + 1e-7)\n",
        "\n",
        "    # Shape: [B, C, L] → [T, B, C, L]\n",
        "    spk_data = spikegen.latency(data, num_steps=num_steps, normalize=True, linear=True)\n",
        "    return spk_data  # [T, B, C, L]"
      ],
      "metadata": {
        "id": "HNo0LwAHVEBx"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conv1D SNN Architecture"
      ],
      "metadata": {
        "id": "P-DO0HMtVMqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SNNConv1D(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        beta = 0.9  # LIF decay constant\n",
        "        self.conv1 = nn.Conv1d(20, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.lif1 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.lif2 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
        "\n",
        "        # self.fc1 = nn.Linear(64 * 20, num_classes)\n",
        "        self.fc1 = nn.Linear(64 * 101, num_classes)\n",
        "        self.num_steps = 100\n",
        "\n",
        "    def forward(self, x):\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        spk_out = 0\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            input_t = x[step]  # Shape: [B x input_dim] [B, 20, T]\n",
        "            # input_t = input_t.unsqueeze(1)  # Add channel dim → [B x 1 x L]\n",
        "            x1 = self.conv1(input_t)\n",
        "            spk1, _ = self.lif1(x1)\n",
        "\n",
        "            x2 = self.conv2(spk1)\n",
        "            spk2, _ = self.lif2(x2)\n",
        "\n",
        "            x_flat = spk2.view(spk2.size(0), -1)\n",
        "            # print(\"Flattened shape:\", x_flat.shape)\n",
        "            out = self.fc1(x_flat)\n",
        "            spk_out += out\n",
        "        return spk_out / self.num_steps  # Soft output across time\n"
      ],
      "metadata": {
        "id": "WrlrYa0-VOqc"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "8EONjxOlVW6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SNNConv1D(num_classes=len(keywords)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for x, y in loader:\n",
        "        # x = transform(x).to(device)  # Apply MFCC transform\n",
        "        spk_x = encode_input(x, num_steps=model.num_steps).to(device)  # [T x B x L] -> TTFS encoding\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(spk_x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(dim=1) == y).sum().item()\n",
        "    acc = correct / len(loader.dataset)\n",
        "    return total_loss / len(loader), acc\n"
      ],
      "metadata": {
        "id": "w5kJll9HVZZv"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Evaluate"
      ],
      "metadata": {
        "id": "jElQFa0EVcSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Accuracy={train_acc*100:.2f}%\")\n",
        "    if train_acc > 0.90:\n",
        "        print(\"Target accuracy reached!\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "pFQmq0eZVfmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}