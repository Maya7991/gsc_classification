{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maya7991/gsc_classification/blob/main/snn_conv1d_mfcc_val.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUJCgZgiVp5W"
      },
      "source": [
        "# SNN conv1D on MFCC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbeYIsj8VnEt"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXeIc6I4UtCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2255de55-5e50-4691-c10f-a886fa7df223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install snntorch --quiet\n",
        "!pip install torchaudio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6t4lE-vU32K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA7BCq3vEmxF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "from snntorch import spikegen, surrogate, functional as SF\n",
        "import snntorch as snn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dTnG_4pU7Ao"
      },
      "source": [
        "### Load & Preprocess the Speech Command Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHMX5TFXU-dk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f9f50f-e234-49c3-9aaa-944c2f18f1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:23<00:00, 104MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform = torchaudio.transforms.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=40,\n",
        "    melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 40}\n",
        ")\n",
        "\n",
        "train_dataset = SPEECHCOMMANDS(\n",
        "    \"./\", download=True, subset=\"training\")\n",
        "val_dataset = SPEECHCOMMANDS(\n",
        "    \"./\", download=True, subset=\"validation\")\n",
        "test_dataset = SPEECHCOMMANDS(\n",
        "    \"./\", download=True, subset=\"testing\")\n",
        "\n",
        "# Limit to a few keywords for now (e.g., \"yes\", \"no\", \"up\", \"down\")\n",
        "keywords = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "label_dict = {k: i for i, k in enumerate(keywords)}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    X, y = [], []\n",
        "    max_len = 0\n",
        "    mfccs = []\n",
        "\n",
        "    for waveform, sample_rate, label, *_ in batch:\n",
        "        if label in keywords:\n",
        "            mfcc = transform(waveform).squeeze(0)  # [n_mfcc, time]\n",
        "            mfccs.append(mfcc)\n",
        "            # print(\"MFCC shape:\", mfcc.shape)  # add this\n",
        "            y.append(label_dict[label])\n",
        "            max_len = max(max_len, mfcc.shape[1])\n",
        "\n",
        "    for mfcc in mfccs:\n",
        "        pad_len = max_len - mfcc.shape[1]\n",
        "        padded = F.pad(mfcc, (0, pad_len))  # Pad on the time dimension (right side)\n",
        "        X.append(padded)\n",
        "\n",
        "    return torch.stack(X), torch.tensor(y)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq3EDG8tVCm_"
      },
      "source": [
        "### Latency encoding\n",
        "\n",
        "encode mfcc features to spike trains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNo0LwAHVEBx"
      },
      "outputs": [],
      "source": [
        "# def encode_input(mfcc_batch, num_steps=100):\n",
        "#     # Normalize to [0, 1]\n",
        "#     data = (mfcc_batch - mfcc_batch.min()) / (mfcc_batch.max() - mfcc_batch.min())\n",
        "#     # [B x C x L] → [B x L] if needed\n",
        "#     data = data.mean(dim=1) if data.ndim == 3 else data\n",
        "#     # Apply latency encoding\n",
        "#     spk_data = spikegen.latency(data, num_steps=num_steps, normalize=True, linear=True)\n",
        "#     return spk_data  # shape: [T x B x L]\n",
        "\n",
        "def encode_input(mfcc_batch, num_steps=15):\n",
        "    # Normalize to [0, 1] per sample\n",
        "    min_val = mfcc_batch.amin(dim=(1,2), keepdim=True)\n",
        "    max_val = mfcc_batch.amax(dim=(1,2), keepdim=True)\n",
        "    data = (mfcc_batch - min_val) / (max_val - min_val + 1e-7)\n",
        "\n",
        "    # Shape: [B, C, L] → [T, B, C, L]\n",
        "    spk_data = spikegen.latency(data, num_steps=num_steps, normalize=True, linear=True)\n",
        "    return spk_data  # [T, B, C, L]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DO0HMtVMqJ"
      },
      "source": [
        "### Conv1D SNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrlrYa0-VOqc"
      },
      "outputs": [],
      "source": [
        "class SNNConv1D(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        beta = 0.9  # LIF decay constant\n",
        "        self.conv1 = nn.Conv1d(40, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.lif1 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.lif2 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
        "\n",
        "        # self.fc1 = nn.Linear(64 * 20, num_classes)\n",
        "        self.fc1 = nn.Linear(64 * 101, num_classes)\n",
        "        self.num_steps = 100\n",
        "\n",
        "    def forward(self, x):\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        spk_out = 0\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            input_t = x[step]  # Shape: [B x input_dim] [B, 20, T]\n",
        "            # input_t = input_t.unsqueeze(1)  # Add channel dim → [B x 1 x L]\n",
        "            x1 = self.conv1(input_t)\n",
        "            spk1, _ = self.lif1(x1)\n",
        "\n",
        "            x2 = self.conv2(spk1)\n",
        "            spk2, _ = self.lif2(x2)\n",
        "\n",
        "            x_flat = spk2.view(spk2.size(0), -1)\n",
        "            # print(\"Flattened shape:\", x_flat.shape)\n",
        "            out = self.fc1(x_flat)\n",
        "            spk_out += out\n",
        "        return spk_out / self.num_steps  # Soft output across time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EONjxOlVW6N"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5kJll9HVZZv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SNNConv1D(num_classes=len(keywords)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for x, y in loader:\n",
        "        # x = transform(x).to(device)  # Apply MFCC transform\n",
        "        spk_x = encode_input(x, num_steps=model.num_steps).to(device)  # [T x B x L] -> TTFS encoding\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(spk_x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(dim=1) == y).sum().item()\n",
        "    acc = correct / len(loader.dataset)\n",
        "    return total_loss / len(loader), acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jElQFa0EVcSR"
      },
      "source": [
        "### Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            spk_x = encode_input(x, num_steps=model.num_steps).to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            out = model(spk_x)\n",
        "            loss = loss_fn(out, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (out.argmax(dim=1) == y).sum().item()\n",
        "    acc = correct / len(loader.dataset)\n",
        "    return total_loss / len(loader), acc"
      ],
      "metadata": {
        "id": "ngE58IHh3JxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pFQmq0eZVfmG"
      },
      "outputs": [],
      "source": [
        "csv_filename = \"snn_mfcc_log.csv\"\n",
        "model_dir = \"checkpoints\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Write header\n",
        "with open(csv_filename, mode='w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"])\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader)\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "    # Log to CSV\n",
        "    with open(csv_filename, mode='a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([epoch, train_loss, train_acc, val_loss, val_acc])\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    # Save model\n",
        "    # model_path = os.path.join(model_dir, f\"snn_epoch_{epoch}.pth\")\n",
        "    # torch.save(model.state_dict(), model_path)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        save_model_path = os.path.join(model_dir, \"snn_conv1d_mfcc_model.pth\")\n",
        "        torch.save(model.state_dict(), save_model_path)\n",
        "        print(f\"Model saved at epoch {epoch} with Val Acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Accuracy={train_acc*100:.2f}%\")\n",
        "    if train_acc > 0.90:\n",
        "        print(\"Target accuracy reached!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jpqgWihH2-jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_GgBOSic29C-"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZbeYIsj8VnEt"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNlfS0bHjwvBnXztE3RzxXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}