{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnTHyM79zUwbYNCZQVlpMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maya7991/gsc_classification/blob/main/snn_conv2d_mfcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch --quiet\n",
        "!pip install torchaudio --quiet"
      ],
      "metadata": {
        "id": "gD54P59WT2uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "from snntorch import spikegen, surrogate, functional as SF\n",
        "import snntorch as snn"
      ],
      "metadata": {
        "id": "iMewRLptT0M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsOC1eYATx8_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_steps = 20\n",
        "encoding_type = \"rate\"  # <<< \"rate\" or \"latency\"\n",
        "save_dir = \"checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "csv_log = \"training_log.csv\"\n",
        "\n",
        "# --- Data ---\n",
        "transform = torchaudio.transforms.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=40,\n",
        "    melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 40}\n",
        ")\n",
        "\n",
        "train_dataset = SPEECHCOMMANDS(\"./\", download=True, subset=\"training\")\n",
        "val_dataset = SPEECHCOMMANDS(\"./\", download=True, subset=\"validation\")\n",
        "test_dataset = SPEECHCOMMANDS(\"./\", download=True, subset=\"testing\")\n",
        "\n",
        "# Build label encoder\n",
        "all_labels = sorted(set(datapoint[2] for datapoint in train_dataset + val_dataset + test_dataset))\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "\n",
        "# Collate function\n",
        "def collate_fn(batch):\n",
        "    X, y = [], []\n",
        "    max_len = 0\n",
        "    mfccs = []\n",
        "\n",
        "    for waveform, sample_rate, label, *_ in batch:\n",
        "        mfcc = transform(waveform).squeeze(0)  # [n_mfcc, time]\n",
        "        mfccs.append(mfcc)\n",
        "        y.append(label_encoder.transform([label])[0])\n",
        "        max_len = max(max_len, mfcc.shape[1])\n",
        "\n",
        "    for mfcc in mfccs:\n",
        "        pad_len = max_len - mfcc.shape[1]\n",
        "        padded = F.pad(mfcc, (0, pad_len))  # Pad on time dimension\n",
        "        X.append(padded.unsqueeze(0))  # [1, n_mfcc, time]\n",
        "\n",
        "    return torch.stack(X), torch.tensor(y)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# --- Spike Encoding ---\n",
        "def encode_input(batch, num_steps, encoding=\"rate\"):\n",
        "    min_val = batch.amin(dim=[2, 3], keepdim=True)\n",
        "    max_val = batch.amax(dim=[2, 3], keepdim=True)\n",
        "    norm_batch = (batch - min_val) / (max_val - min_val + 1e-7)\n",
        "\n",
        "    if encoding == \"rate\":\n",
        "        spk = spikegen.rate(norm_batch, num_steps=num_steps)\n",
        "    elif encoding == \"latency\":\n",
        "        spk = spikegen.latency(norm_batch, num_steps=num_steps, normalize=True, linear=True)\n",
        "    else:\n",
        "        raise ValueError(\"Encoding must be 'rate' or 'latency'\")\n",
        "    return spk  # [T, B, C, H, W]\n",
        "\n",
        "# --- Model ---\n",
        "class SNN_Conv2D(nn.Module):\n",
        "    def __init__(self, num_classes, num_steps):\n",
        "        super().__init__()\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.lif1 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.lif2 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.lif3 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(128 * 10 * 13, num_classes)  # Check your final feature map size!\n",
        "\n",
        "    def forward(self, x):\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        spk_out = 0\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            input_t = x[step]\n",
        "\n",
        "            cur1 = self.conv1(input_t)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "\n",
        "            cur2 = self.conv2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "\n",
        "            cur3 = self.conv3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            flat = self.flatten(spk3)\n",
        "            out = self.fc(flat)\n",
        "\n",
        "            spk_out += out\n",
        "\n",
        "        return spk_out / self.num_steps\n",
        "\n",
        "# --- Training & Evaluation ---\n",
        "def train_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        spk_x = encode_input(x.to(device), model.num_steps, encoding=encoding_type)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(spk_x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(dim=1) == y).sum().item()\n",
        "\n",
        "    acc = correct / len(loader.dataset)\n",
        "    return total_loss / len(loader), acc\n",
        "\n",
        "def evaluate(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            spk_x = encode_input(x.to(device), model.num_steps, encoding=encoding_type)\n",
        "            y = y.to(device)\n",
        "\n",
        "            out = model(spk_x)\n",
        "            loss = loss_fn(out, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (out.argmax(dim=1) == y).sum().item()\n",
        "\n",
        "    acc = correct / len(loader.dataset)\n",
        "    return total_loss / len(loader), acc\n",
        "\n",
        "# --- Instantiate Model ---\n",
        "num_classes = len(label_encoder.classes_)\n",
        "model = SNN_Conv2D(num_classes=num_classes, num_steps=num_steps).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "with open(csv_log, mode='w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"])\n",
        "\n",
        "best_val_acc = 0.0\n",
        "num_epochs = 30\n",
        "\n",
        "print(\"Start training!\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n",
        "\n",
        "    # Log\n",
        "    with open(csv_log, mode='a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([epoch, train_loss, train_acc, val_loss, val_acc])\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
        "        print(f\"Saved model at epoch {epoch}!\")\n",
        "\n",
        "# --- Final Test Evaluation ---\n",
        "test_loss, test_acc = evaluate(model, test_loader, loss_fn)\n",
        "print(f\"Test Loss={test_loss:.4f}, Acc={test_acc*100:.2f}%\")\n"
      ]
    }
  ]
}