{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWXOLO8Qc98tECZREKTWxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maya7991/gsc_classification/blob/main/fmnist_snn_multi_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pure SNN  conv architecture to train FMNIST with ttfs/rate encoding\n",
        "\n",
        "*   Option to choose rate or latency (TTFS) encoding of input\n",
        "*   The loss function is applied based on the encoding scheme used\n",
        "*   model with least val loss saved\n",
        "*   loss and acc during training saved to csv\n",
        "\n",
        "* the architecture or model is not stable. This notebook is just a framework for using different encoding schemes for training. Model can be improved by some hyperparameter training\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kti6XJ8W5TJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Up4WnffSXUlo"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms,datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "from snntorch import spikegen\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDcl859dXqJK",
        "outputId": "3e9a904f-685e-4484-e583-91d514f348a1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28,28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST('data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training into train/val\n",
        "train_len = int(0.9 * len(train_dataset))\n",
        "val_len = len(train_dataset) - train_len\n",
        "train_data, val_data = random_split(train_dataset, [train_len, val_len])\n",
        "\n",
        "batch_size=50\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "print(f\"Training data size : {len(train_data)}, Validation data size : {len(val_data)}, Test data size : {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bAxU38mXwzN",
        "outputId": "dede2ce2-537b-4752-df90-c25dca270973"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size : 54000, Validation data size : 6000, Test data size : 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define spiking CNN\n",
        "# class SNNConvNet(nn.Module):\n",
        "#     def __init__(self, num_steps, encoding):\n",
        "#         super().__init__()\n",
        "#         beta=0.9\n",
        "#         spike_grad = surrogate.fast_sigmoid()\n",
        "#         self.num_steps = num_steps\n",
        "#         self.encoding = encoding\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(1, 8, 5)\n",
        "#         self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
        "#         self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "#         self.conv2 = nn.Conv2d(8, 16, 3)\n",
        "#         self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
        "#         self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "#         self.fc1 = nn.Linear(16 * 25, 128)\n",
        "#         self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
        "#         self.fc2 = nn.Linear(128, 10)\n",
        "#         self.lif4 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         spk4_rec = []\n",
        "\n",
        "#         if encoding == \"rate\":\n",
        "#             x = spikegen.rate(x, num_steps=num_steps)\n",
        "#         elif encoding == \"ttfs\":\n",
        "#             # x = spikegen.ttfs(x, num_steps=num_steps)\n",
        "#             spikegen.latency(x, num_steps=num_steps, normalize=True, linear=True)\n",
        "#         else:\n",
        "#             raise ValueError(\"Encoding must be 'rate' or 'ttfs'\")\n",
        "\n",
        "#         self.lif1.init_leaky()\n",
        "#         self.lif2.init_leaky()\n",
        "#         self.lif3.init_leaky()\n",
        "#         self.lif4.init_leaky()\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             cur1 = self.conv1(x[step])\n",
        "#             spk1 = self.lif1(cur1)\n",
        "#             smp1 = self.pool1(spk1)\n",
        "\n",
        "#             cur2 = self.conv2(smp1)\n",
        "#             spk2= self.lif2(cur2)\n",
        "#             smp2 = self.pool2(spk2)\n",
        "\n",
        "#             flat = smp2.view(smp2.size(0), -1)\n",
        "#             cur3 = self.fc1(flat)\n",
        "#             spk3= self.lif3(cur3)\n",
        "\n",
        "#             cur4 = self.fc2(spk3)\n",
        "#             spk4= self.lif4(cur4)\n",
        "\n",
        "#             spk4_rec.append(spk4)\n",
        "\n",
        "#         return torch.stack(spk4_rec, dim=0)"
      ],
      "metadata": {
        "id": "OMf9TEkaaNAs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_steps, encoding):\n",
        "        super().__init__()\n",
        "        beta=0.9\n",
        "        spike_grad = surrogate.fast_sigmoid(slope=25)\n",
        "        self.num_steps = num_steps\n",
        "        self.encoding = encoding\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1 = nn.Conv2d(1, 12, 5)\n",
        "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
        "        self.conv2 = nn.Conv2d(12, 32, 5)\n",
        "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
        "        self.fc1 = nn.Linear(512, 10)\n",
        "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk_rec = []\n",
        "        if encoding == \"rate\":\n",
        "            x = spikegen.rate(x, num_steps=num_steps)\n",
        "        elif encoding == \"ttfs\":\n",
        "            x = spikegen.latency(x, num_steps=num_steps, normalize=True, linear=True)\n",
        "        else:\n",
        "            raise ValueError(\"Encoding must be 'rate' or 'ttfs'\")\n",
        "\n",
        "        # Initialize hidden states and outputs at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "          cur1 = F.max_pool2d(self.conv1(x[step]), 2)\n",
        "          spk1, mem1 = self.lif1(cur1, mem1)\n",
        "\n",
        "          cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "          spk2, mem2 = self.lif2(cur2, mem2)\n",
        "\n",
        "          cur3 = self.fc1(spk2.view(spk2.size(0), -1))\n",
        "          # cur3 = self.fc1(spk2.view(batch_size, -1))\n",
        "          spk3, mem3 = self.lif3(cur3, mem3)\n",
        "          spk_rec.append(spk3)\n",
        "\n",
        "        return torch.stack(spk_rec, dim=0)"
      ],
      "metadata": {
        "id": "em83dWnZ8zKx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/My Drive/thesis_apr'\n",
        "num_epochs = 15\n",
        "learning_rate = 1e-4\n",
        "num_steps = 20\n",
        "encoding= \"ttfs\"\n",
        "save_model_name = f\"fmnist_snn_conv_{encoding}.pth\"\n",
        "\n",
        "if encoding == \"rate\":\n",
        "    loss_fn = SF.ce_rate_loss()\n",
        "elif encoding == \"ttfs\":\n",
        "    loss_fn = SF.ce_temporal_loss()\n",
        "else:\n",
        "    raise ValueError(\"Encoding must be 'rate' or 'ttfs'\")\n",
        "\n",
        "net = Net(num_steps=num_steps, encoding=encoding).to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "def trainNet(model, optimizer, encoding, epochs):\n",
        "    print(loss_fn)\n",
        "    loss_keeper={'train_loss':[],'valid_loss':[],'train_acc':[],'valid_acc':[]}\n",
        "    val_loss_min = np.inf\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      train_loss=0.0\n",
        "      train_acc = 0.0\n",
        "      val_loss=0.0\n",
        "      val_acc = 0.0\n",
        "\n",
        "      \"\"\"\n",
        "      TRAINING PHASE\n",
        "      \"\"\"\n",
        "      model.train()\n",
        "      for data, targets in train_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        spk_out = net(data)\n",
        "\n",
        "        loss = loss_fn(spk_out, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += SF.accuracy_temporal(spk_out, targets)\n",
        "\n",
        "    # return total_loss / len(train_loader), total_acc / len(train_loader)\n",
        "      \"\"\"\n",
        "      VALIDATION PHASE\n",
        "      \"\"\"\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            spk_out = net(data)\n",
        "            loss = loss_fn(spk_out, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += SF.accuracy_temporal(spk_out, targets)\n",
        "\n",
        "      train_loss = train_loss / len(train_loader)\n",
        "      train_acc = 100*train_acc / len(train_loader)\n",
        "      val_loss = val_loss / len(val_loader)\n",
        "      val_acc = 100* val_acc / len(val_loader)\n",
        "      loss_keeper['train_loss'].append(train_loss)\n",
        "      loss_keeper['valid_loss'].append(val_loss)\n",
        "      loss_keeper['train_acc'].append(train_acc)\n",
        "      loss_keeper['valid_acc'].append(val_acc)\n",
        "\n",
        "      print(f\"\\nEpoch : {epoch+1}\\tTraining Loss : {train_loss:.4f}\\t Validation Loss : {val_loss:.4f}\\t Training Acc : {train_acc:.4f}% \\tValidation Acc: {val_acc:.4f}%\")\n",
        "\n",
        "      if val_loss<=val_loss_min:\n",
        "            print(f\"Validation loss decreased from : {val_loss_min} ----> {val_loss} ----> Saving Model.......\")\n",
        "            torch.save(model.state_dict(), base_path + '/model/fmnist/' + save_model_name) # saving entire model causes PicklingError\n",
        "            val_loss_min=val_loss\n",
        "\n",
        "    return loss_keeper\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    total_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            spk_rec = model(data)\n",
        "            acc = SF.accuracy_temporal(spk_rec, targets)\n",
        "            total_acc += acc.item()\n",
        "\n",
        "    print(f\"Test Accuracy: {100*total_acc/len(test_loader):.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    loss_keeper = trainNet(net, optimizer, encoding, num_epochs )\n",
        "    test(net)\n",
        "\n",
        "    # Save loss and accuracy to CSV\n",
        "    results_df = pd.DataFrame(loss_keeper)\n",
        "    csv_save_path = os.path.join(base_path, 'logs', f'loss_acc_{encoding}.csv')\n",
        "    os.makedirs(os.path.dirname(csv_save_path), exist_ok=True)\n",
        "    results_df.to_csv(csv_save_path, index=False)\n",
        "    print(f\"Training log saved to: {csv_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbXkgxuHdDN4",
        "outputId": "5423f855-142f-42c4-b2ed-7b7ebd8a9812"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch : 1\tTraining Loss : 1.7867\t Validation Loss : 1.4944\t Training Acc : 44.9167% \tValidation Acc: 53.7333%\n",
            "Validation loss decreased from : inf ----> 1.4943573450048764 ----> Saving Model.......\n",
            "\n",
            "Epoch : 2\tTraining Loss : 1.5101\t Validation Loss : 1.4246\t Training Acc : 48.1500% \tValidation Acc: 48.3167%\n",
            "Validation loss decreased from : 1.4943573450048764 ----> 1.424613740046819 ----> Saving Model.......\n",
            "\n",
            "Epoch : 3\tTraining Loss : 1.4249\t Validation Loss : 1.3365\t Training Acc : 50.1574% \tValidation Acc: 50.4667%\n",
            "Validation loss decreased from : 1.424613740046819 ----> 1.3364959860841432 ----> Saving Model.......\n",
            "\n",
            "Epoch : 4\tTraining Loss : 1.2328\t Validation Loss : 1.2273\t Training Acc : 56.8500% \tValidation Acc: 58.1000%\n",
            "Validation loss decreased from : 1.3364959860841432 ----> 1.2272872666517893 ----> Saving Model.......\n",
            "\n",
            "Epoch : 5\tTraining Loss : 1.0522\t Validation Loss : 1.0769\t Training Acc : 65.8259% \tValidation Acc: 66.6500%\n",
            "Validation loss decreased from : 1.2272872666517893 ----> 1.0768627340594927 ----> Saving Model.......\n",
            "\n",
            "Epoch : 6\tTraining Loss : 0.9833\t Validation Loss : 0.9001\t Training Acc : 68.3463% \tValidation Acc: 71.3500%\n",
            "Validation loss decreased from : 1.0768627340594927 ----> 0.9001331808666388 ----> Saving Model.......\n",
            "\n",
            "Epoch : 7\tTraining Loss : 0.9725\t Validation Loss : 0.9205\t Training Acc : 68.1056% \tValidation Acc: 68.7167%\n",
            "\n",
            "Epoch : 8\tTraining Loss : 0.9080\t Validation Loss : 0.8800\t Training Acc : 70.6148% \tValidation Acc: 73.0833%\n",
            "Validation loss decreased from : 0.9001331808666388 ----> 0.8800076966484388 ----> Saving Model.......\n",
            "\n",
            "Epoch : 9\tTraining Loss : 0.7215\t Validation Loss : 0.8686\t Training Acc : 77.5333% \tValidation Acc: 74.4333%\n",
            "Validation loss decreased from : 0.8800076966484388 ----> 0.8686134964227676 ----> Saving Model.......\n",
            "\n",
            "Epoch : 10\tTraining Loss : 0.6572\t Validation Loss : 0.7024\t Training Acc : 79.4148% \tValidation Acc: 78.1000%\n",
            "Validation loss decreased from : 0.8686134964227676 ----> 0.702408338089784 ----> Saving Model.......\n",
            "Test Accuracy: 77.8900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ixp-0xerKVC3"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}